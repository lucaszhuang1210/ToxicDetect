# ToxicDetect - Advanced Toxic Comment Classifier

**[Final Project Report (PDF)](./Advancing_Toxic_Comment_Classification.pdf)**  
*Advancing Toxic Comment Classification: A Comparative Study of TF-IDF, SBERT, and Deep Learning Models*  

---

A **machine learning pipeline** for detecting and classifying toxic comments across multiple categories.

This project investigates multiple approaches to **multi-label toxic comment classification** using the [Kaggle Toxic Comment Challenge dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). We compare traditional machine learning and deep learning pipelines to balance **performance, interpretability, and computational efficiency** for better content moderation.  

---

## Modeling Approaches  

Explored methods include:  
1. **TF-IDF + Naive Bayes** â€“ baseline for imbalanced text data.  
2. **TF-IDF + Logistic Regression** â€“ tuned linear model for improved performance.  
3. **SBERT + Logistic Regression / XGBoost** â€“ semantic embeddings for richer representations.  
4. **RoBERTa Fine-Tuning** â€“ end-to-end deep learning with contextualized features.  

Our objective: **evaluate and compare traditional vs. deep learning approaches** to identify the optimal strategy for scalable, reliable toxic language detection.  

---

## Result Summary  
- **Naive Bayes + TF-IDF**: ROC-AUC â‰ˆ 0.945 â€” strong baseline, but limited flexibility.  
- **Logistic Regression + TF-IDF**: ROC-AUC â‰ˆ 0.976 â€” best traditional model, efficient and interpretable.  
- **SBERT + Logistic Regression**: ROC-AUC â‰ˆ 0.969 â€” better semantic representation, but only modest gain.  
- **SBERT + XGBoost**: ROC-AUC â‰ˆ 0.959 â€” leveraging non-linear boosting, but not top performer.  
- **RoBERTa Fine-Tuning**: ROC-AUC â‰ˆ 0.978 â€” highest score, best contextual understanding, but computationally expensive.  

**Conclusion:** TF-IDF with Logistic Regression remains a highly effective baseline, but fine-tuned transformer models (e.g., RoBERTa) provide the best performance when resources allow.  

---

## ðŸ“‚ Project Structure  
```
.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ sample_submission.csv
â”‚   â”œâ”€â”€ test_labels.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ train.csv
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ generate_embeddings.py
â”‚   â”œâ”€â”€ RoBERTa.py
â”‚   â”œâ”€â”€ SBERT_LogisticRegression.ipynb
â”‚   â””â”€â”€ TFIDF_LogisticRegression_Optimized.ipynb
â”œâ”€â”€ Advancing Toxic Comment Classification.pdf
â””â”€â”€ README.md
```
